{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "<h1 class=\"title\">Jason Scott's favorite 100 - data analysis using NLP</h1>\n",
    "<h2 class=\"subtitle\">Mini-Project in the ABC of Computational Text Analysis</h2>\n",
    "<p>Author: <span class=\"author\">Josias Bruderer, Universit√§t Luzern</span></p>\n",
    "<p>Date: <span class=\"date\">26. May 2021</span></p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparations [R1]\n",
    "\n",
    "The following lines of code is used for preparing our environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# load the necessary libraries\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import textacy\n",
    "import spacy\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import scattertext as st\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from plotnine import *\n",
    "#import textacy.vsm #this does not work... TODO\n",
    "\n",
    "#run: ./.envs/bin/python -m spacy download en_core_web_sm\n",
    "\n",
    "project_path = Path.cwd().parent\n",
    "\n",
    "# prepare to load project specific libraries\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(str(project_path))\n",
    "\n",
    "# import data_wrangler module\n",
    "from modules import data_wrangler\n",
    "from modules import helpers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "PosixPath('/home/josias/projects/bbs-for-independence/03_workspace')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path.cwd().parent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Wrangling [R2]\n",
    "\n",
    "In this section the required data is downloaded and preprocessed (f.E. unzipped). The module data_wrangler will be used for this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_url = \"http://archives.textfiles.com/100.zip\"\n",
    "data_name = \"100\" #use 100 to only analize the top100 files\n",
    "data_dir = str(project_path.parent / \"02_datasets/\")\n",
    "tmp_dir = str(project_path.parent / \".tmp/\")\n",
    "\n",
    "dw = data_wrangler.DataWrangler(tmp_dir, data_dir)\n",
    "\n",
    "dw.process_zip(data_url, data_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analisis [R3]\n",
    "\n",
    "## Preliminary Clarifications [R3.1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preparation for manual analisis [R3.1.1]\n",
    "\n",
    "# read textfiles\n",
    "dataset = dw.get_texts(data_dir + \"/\" + data_name)\n",
    "\n",
    "# write metadata to csv file\n",
    "f = open(\"top100_generated.csv\", \"w+\")\n",
    "for item in dataset:\n",
    "    f.write(item[1][\"name\"]+\",\"+\n",
    "            str(item[1][\"length\"])+\",\"+\n",
    "            str(item[1][\"length_raw\"])+\",\"+\n",
    "            str(item[1][\"avgcolumnsize\"])+\",\"+\n",
    "            str(item[1][\"charratioA\"])+\",\"+\n",
    "            str(item[1][\"charratioB\"])+\",\"+\n",
    "            str(item[1][\"year\"])+\",\"+\n",
    "            str(item[1][\"eyear\"])+\",\"+\n",
    "            str(item[1][\"lyear\"])+\",\"+\n",
    "            str(item[1][\"type\"])+\",\"+\n",
    "            \"\\r\\n\")\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Textcorpora [R3.2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read textfiles\n",
    "dataset = dw.get_texts(data_dir + \"/\" + data_name)\n",
    "\n",
    "en = textacy.load_spacy_lang(\"en_core_web_sm\")\n",
    "\n",
    "# create corpus from processed documents\n",
    "corpus = textacy.Corpus(en, data=dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Subcorpora [R3.3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to filter by metadata: filter invalid filenames\n",
    "def filter_by_name(doc):\n",
    "    return doc._.meta.get(\"name\") != \"index.html\" and doc._.meta.get(\"name\") != \".ztr-directory\"\n",
    "\n",
    "# function to filter by metadata: filter files that contain less than 50% A-z characters\n",
    "def filter_by_charratio(doc):\n",
    "    return doc._.meta.get(\"charratioA\") > 0.5\n",
    "\n",
    "# create new corpus after applying filter function\n",
    "subcorpustemp = textacy.corpus.Corpus(en, data=corpus.get(filter_by_name))\n",
    "subcorpus = textacy.corpus.Corpus(en, data=subcorpustemp.get(filter_by_charratio))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wordcount [R3.4]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get lowercased and filtered corpus vocabulary (R3.3.1)\n",
    "vocab = subcorpus.word_counts(as_strings=True, normalize= 'lower', filter_stops = True, filter_punct = True, filter_nums = True)\n",
    "\n",
    "# sort vocabulary by descending frequency\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# write to file, one word and its frequency per line\n",
    "fname = './analysis/vocab_frq.txt'\n",
    "with open(fname, 'w') as f:\n",
    "    for word, frq in vocab_sorted:\n",
    "        line = f\"{word}\\t{frq}\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "vocab_sorted[:25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export to CSV [R3.5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subcorpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-6a8d6d7e4d94>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# merge metadata and actual content for each document in the corpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# ugly, verbose syntax to merge two dictionaries\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeta\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mdoc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m}\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msubcorpus\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# export corpus as csv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'subcorpus' is not defined"
     ]
    }
   ],
   "source": [
    "# merge metadata and actual content for each document in the corpus\n",
    "# ugly, verbose syntax to merge two dictionaries\n",
    "data = [{**doc._.meta, **{'text': doc.text}} for doc in subcorpus]\n",
    "\n",
    "# export corpus as csv\n",
    "f_csv = './analysis/subcorpus.csv'\n",
    "textacy.io.csv.write_csv(data, f_csv, fieldnames=data[0].keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load CSV file [R3.6]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read dataset from csv file\n",
    "f_csv = './analysis/subcorpus.csv'\n",
    "df = pd.read_csv(f_csv)\n",
    "\n",
    "df_sub = df[(df['text'].str.len() > 10)]\n",
    "\n",
    "# make new column containing all relevant metadata (showing in plot later on)\n",
    "df_sub['descripton'] = df_sub[['name', 'year', 'charratioA', 'avgcolumnsize']].astype(str).agg(', '.join, axis=1)\n",
    "\n",
    "# sneak peek of dataset\n",
    "df_sub.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scattertext Plot [R3.7]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import with a short name\n",
    "import scattertext as st\n",
    "import pandas as pd\n",
    "\n",
    "# import all specific/all objects from a module\n",
    "from pathlib import Path\n",
    "from plotnine import *\n",
    "import textacy.vsm\n",
    "\n",
    "censor_tags = set(['CARD']) # tags to ignore in corpus, e.g. numbers\n",
    "\n",
    "# stop words to ignore in corpus\n",
    "en_stopwords = spacy.lang.en.stop_words.STOP_WORDS # default stop words\n",
    "custom_stopwords = set(['[', ']', '%'])\n",
    "en_stopwords = en_stopwords.union(custom_stopwords) # extend with custom stop words\n",
    "\n",
    "# create corpus from dataframe\n",
    "# lowercased terms, no stopwords, no numbers\n",
    "# use lemmas for English only, German quality is too bad\n",
    "corpus_speeches = st.CorpusFromPandas(df_sub, # dataset\n",
    "                             category_col='type', # index differences by ...\n",
    "                             text_col='text',\n",
    "                             nlp=en, # German model\n",
    "                             feats_from_spacy_doc=st.FeatsFromSpacyDoc(tag_types_to_censor=censor_tags, use_lemmas=True),\n",
    "                             ).build().get_stoplisted_unigram_corpus(en_stopwords)\n",
    "\n",
    "# produce visualization (interactive html)\n",
    "html = st.produce_scattertext_explorer(corpus_speeches,\n",
    "            category='declaration', # set attribute to divide corpus into two parts\n",
    "            category_name='declaration',\n",
    "            not_category_name='textfiles',\n",
    "            metadata=df_sub['descripton'],\n",
    "            width_in_pixels=1000,\n",
    "            minimum_term_frequency=5, # drop terms occurring less than 5 times\n",
    "            save_svg_button=True,\n",
    ")\n",
    "\n",
    "# write visualization to html file\n",
    "fname = \"../public/viz_declaration_textfiles.html\"\n",
    "open(fname, 'wb').write(html.encode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Docs per Year [R3.8]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtmp = df_sub.groupby('eyear').agg({'text': \"count\" }).reset_index().rename(columns={'text':'count'})\n",
    "dtmp = dtmp.rename(columns={\"eyear\": \"year\"})\n",
    "dtmp.insert(2,\"type\",\"eyear\")\n",
    "docs_per_year = dtmp\n",
    "\n",
    "dtmp = df_sub.groupby('lyear').agg({'text': \"count\" }).reset_index().rename(columns={'text':'count'})\n",
    "dtmp = dtmp.rename(columns={\"lyear\": \"year\"})\n",
    "dtmp.insert(2,\"type\",\"lyear\")\n",
    "docs_per_year = docs_per_year.append(dtmp, ignore_index=True)\n",
    "\n",
    "dtmp = pd.read_csv('top100_years.txt', delimiter = \",\").groupby('myear').agg({'text': \"count\" }).reset_index().rename(columns={'text':'count'})\n",
    "dtmp = dtmp.rename(columns={\"myear\": \"year\"})\n",
    "dtmp.insert(2,\"type\",\"myear\")\n",
    "docs_per_year = docs_per_year.append(dtmp, ignore_index=True)\n",
    "\n",
    "docs_per_year = docs_per_year[docs_per_year[\"year\"] != \"nd.\"]\n",
    "docs_per_year['year'] = pd.to_numeric(docs_per_year['year'])\n",
    "\n",
    "(ggplot(docs_per_year, aes('year', 'count', color='type', group='type'))\n",
    " + geom_point(alpha=0.5, stroke = 0)\n",
    " + geom_line()\n",
    " + theme_classic()\n",
    " + labs(x = \"Year\",\n",
    "        y = \"absolute number\",\n",
    "        color = \"Legend\")\n",
    " + theme(axis_text_x = element_text(angle = 90, hjust = 1))\n",
    " + scale_x_continuous(limits = (1960,1999))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlations for docs per year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sorry for ugly...\n",
    "dummy = pd.DataFrame([{\"year\":1960, \"count\": 0},\n",
    "                     {\"year\":1961, \"count\": 0},\n",
    "                     {\"year\":1962, \"count\": 0},\n",
    "                     {\"year\":1963, \"count\": 0},\n",
    "                     {\"year\":1964, \"count\": 0},\n",
    "                     {\"year\":1965, \"count\": 0},\n",
    "                     {\"year\":1966, \"count\": 0},\n",
    "                     {\"year\":1967, \"count\": 0},\n",
    "                     {\"year\":1968, \"count\": 0},\n",
    "                     {\"year\":1969, \"count\": 0},\n",
    "                     {\"year\":1970, \"count\": 0},\n",
    "                     {\"year\":1971, \"count\": 0},\n",
    "                     {\"year\":1972, \"count\": 0},\n",
    "                     {\"year\":1973, \"count\": 0},\n",
    "                     {\"year\":1974, \"count\": 0},\n",
    "                     {\"year\":1975, \"count\": 0},\n",
    "                     {\"year\":1976, \"count\": 0},\n",
    "                     {\"year\":1977, \"count\": 0},\n",
    "                     {\"year\":1978, \"count\": 0},\n",
    "                     {\"year\":1979, \"count\": 0},\n",
    "                     {\"year\":1980, \"count\": 0},\n",
    "                     {\"year\":1981, \"count\": 0},\n",
    "                     {\"year\":1982, \"count\": 0},\n",
    "                     {\"year\":1983, \"count\": 0},\n",
    "                     {\"year\":1984, \"count\": 0},\n",
    "                     {\"year\":1985, \"count\": 0},\n",
    "                     {\"year\":1986, \"count\": 0},\n",
    "                     {\"year\":1987, \"count\": 0},\n",
    "                     {\"year\":1988, \"count\": 0},\n",
    "                     {\"year\":1989, \"count\": 0},\n",
    "                     {\"year\":1990, \"count\": 0},\n",
    "                     {\"year\":1991, \"count\": 0},\n",
    "                     {\"year\":1992, \"count\": 0},\n",
    "                     {\"year\":1993, \"count\": 0},\n",
    "                     {\"year\":1994, \"count\": 0},\n",
    "                     {\"year\":1995, \"count\": 0},\n",
    "                     {\"year\":1996, \"count\": 0},\n",
    "                     {\"year\":1997, \"count\": 0},\n",
    "                     {\"year\":1998, \"count\": 0},\n",
    "                     {\"year\":1999, \"count\": 0}])\n",
    "\n",
    "m = dummy.append(docs_per_year[docs_per_year['type'] == \"myear\"][[\"year\",\"count\"]]).groupby('year').agg({'count': \"sum\" })\n",
    "e = dummy.append(docs_per_year[docs_per_year['type'] == \"eyear\"][[\"year\",\"count\"]]).groupby('year').agg({'count': \"sum\" })\n",
    "l = dummy.append(docs_per_year[docs_per_year['type'] == \"lyear\"][[\"year\",\"count\"]]).groupby('year').agg({'count': \"sum\" })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"r_{myear mit eyear} = \",np.corrcoef(m[\"count\"], e[\"count\"])[0, 1])\n",
    "print(\"r_{myear mit lyear} = \",np.corrcoef(m[\"count\"], l[\"count\"])[0, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entities [R3.9]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entities = []\n",
    "\n",
    "for doc in subcorpus.docs:\n",
    "    for ent in textacy.extract.entities(doc):\n",
    "        try:\n",
    "            entities += [{\"text\": ent.text, \"label\": ent.label_, \"explain\":spacy.explain(ent.label_)}]\n",
    "        except:\n",
    "            print(\"Problem with:\", doc._.meta[\"name\"])\n",
    "\n",
    "# export corpus as csv\n",
    "f_csv = './analysis/entities.csv'\n",
    "textacy.io.csv.write_csv(entities, f_csv, fieldnames=entities[0].keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_sub.groupby('eyear').agg({'text': \"count\" }).reset_index().rename(columns={'text':'count'})\n",
    "\n",
    "df_entities = pd.DataFrame(entities,columns=['text','label','explain'])\n",
    "df_entities_count = df_entities.groupby('text').agg({'label': \"count\"}).rename(columns={'label':'count'}).sort_values(by=['count'],ascending=False).reset_index()\n",
    "\n",
    "\n",
    "# write to file, one word and its frequency per line\n",
    "fname = './analysis/entities_frq.csv'\n",
    "with open(fname, 'w') as f:\n",
    "    for i, d in df_entities_count.iterrows():\n",
    "        line = d[\"text\"]+\",\"+str(d[\"count\"])+\"\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "df_entities_count[:25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}