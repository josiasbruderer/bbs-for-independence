---
title: "03_Methode"
subject: ""
id: 20210719163622
lang: de-CH
author:
  - name: Josias Bruderer
    affiliation:  Universität Luzern
keywords:
  - unilu
topics:
   - 
description: ""
---

# Methode zur Untersuchung von textfiles.com

Für diese Arbeit konnte ein Archiv mit BBS Text-Dateien ausfindig gemacht werden. Es wird von Jason Scott unterhalten, der nebst Archivar und Historiker Mitwirkender bei archive.org sowie Regiseur vom Film «BBS: The Documentary» ist. Der Datensatz erscheint aufgrund seines Umfanges ($N=58'0000$ Text-Dateien) und Scotts Hintergrund als geeignet, um eine Analyse von BBS Inhalten durchzuführen. Ebenfalls steht ein verkleinerter Datensatz («favorite 100») zur Verfügung, der für die Analyse hilfreich ist.

Die Analyse erfolgt in drei Schritten: Im ersten Schritt wird der verkleinerte Datensatz manuell auf inhaltliche Auffälligkeiten untersucht und daran ein Modell zur Bereinigung des gesammten Datensatzes entwickelt.[^7] Anschliessend wird im zweiten Schritt der gesammte Datensatz bereinigt. Abschliessend wird mithilfe von Techniken der NLP der bereinigte Datensatz untersucht und damit die Fragestellung zu beantworten versucht.   

## Überblick

Im verkleinerten Datensatz, «a ‹best of› collection of one hundred textfiles that [Scott] think[s] capture the spirit of this site and the unique culture that it attempts to preserve» [@jasonscottJasonScottTop], werden Auffälligkeiten zu Jahr, Länge, Struktur und Inhalt festgehalten und fliessen in den folgenden Abschnitt *Datenbereinigung* ein. Wichtig erscheint an dieser Stelle, dass es sich um durchgehend unstrukturierte Texte handelt. Auch sind die Daten trotz derer Menge «Ergebnisse historisch kontingenter Entscheidungen, wie Auswahl, Formatierung, Kombination etc. und als solche weder neutral, transparent oder objektiv, sondern immer konstruiert» [@muetzelSchoeneDatenKonstruktion2018, 112]. Dies ist für die Auswertung und das Ziehen von Schlüssen sehr relevant. Die folgenden Punkte geben einen besseren Überblick über den Datensatz:[^8]

1. Die **Variation** der Textdateien ist relativ gross und reicht von kurzen witzigen Beiträgen und Unterhaltungsverläufen, über ASCII Art bis hin zu detaillierten technischen Instruktionen und Dokumentationen sowie einer Masterarbeit und einem ganzes Buch.
2. In gewissen Textdateien wird ein **«Read X times»** ausgewiesen. Diese Zahlen sind relativ niedrig (meist <100). 
3. **Ungültige Zeichen** kommen häufig vor (z.B. «\\u1a\\u1a\\u1a»). Anhäufungen von **Sonderzeichen** dienen zur Formatierung.
4. Textdateien mit E-Mail Charakteristik können anhand **Headerparameter** wie z.B. *From, Subject, Date, Organization* erkannt werden.
5. Verschiedene **Datumsformate** sind zu finden.
6. Es kann nicht davon ausgegangen werden, dass die Textdateien **orthografisch** fehlerfrei sind. Ebenfalls ist mit ***Gunk** «(replacing U for You, 0 for O, Z for S, and similar gunk)»* zu rechnen.
7. **Inhaltlich** kommt von sauber recherchierten Artikeln und Facts bis hin zu wilder Fiktion und Ironie alles vor. 
8. Die beim Download von .zip Dateien enthaltene **index.html** repräsentiert keine BBS Textdatei.
9. Textfiles.com führt nebst den Dateien auch Titel (inkl. Jahr wenn vorhanden) und Kategorisierung sowie zum Teil eine Beschreibung auf. Diese **Metadaten** können für die Analyse nützlich sein.

## Datenbereinigung

Aus den aufgelisteten Feststellungen wird nun ein Plan entwickelt, wie die Textfiles bereinigt werden. Das Kapitel «Schöne Daten» von Sophie Mützel et al. [-@muetzelSchoeneDatenKonstruktion2018] macht deutlich, dass die Datenbereinigung oft nur marginal erwähnt wird, obschon diese elementar ist. Die Bereinigung soll für diese Arbeit daher genau durchdacht und transparent dokumentiert werden.

Bereits beim Herunterladen der Textfiles sollen unpassende Dateien (z.B. Bilder oder Audio) exkludiert werden. Gleichzeitig sollen von den verliebenen Dateien entsprechende Metadaten gespeichert werden. Dann erfolgt die erste Bereinigung, nämlich das Entfernen von Formatierungen und anderen nicht relevante Elemente:

* Bereinigung von ungültigen Zeichen (z.B. \\x1a)
* Bereinigung von Zeichen die kein Text repräsentieren (`[^A-z0-9\ \.\'\,\!]` oder `[\\\\\^\[\]]`)
* Bereinigung von Formatierungszeichen (Zeilenumbrüche, Tabulatore) (`[\\r\\n\\t]`)

Anschliessend werden zusätzliche Metadaten generiert:

* Kategorie, Dateiname
* Zeichenanzahl (raw & bereinigt)
* Durchschnittliche Spaltenbreite (raw)
* Anteil von Fliesstext gegenüber Sonderzeichen (raw)
* Jahr des Textfiles (Annahme: zwischen 1960-1999)

Anhand dieser Metadaten kann eine erste Analyse des Datensets gemacht werden.[^10] Einerseits sollen ungeeignete Kategorien bereits zu diesem Zeitpunkt aus der Analyse ausgeschlossen werden, ebenfalls sollen geeignete Parameter für die Reduktion des Datensets gefunden werden. Die Analyse kommt zu folgendem Schluss:

* Folgende Kategorien werden ausgeschlossen, da deren durchschnittliches Verhältnis an sinnvollen Zeichen zur Dateilänge (charratioB) unter 0.8 liegt. Dies stellen Sammlungen von ASCII-Art, Softwarecode, Bilder oder ähnlichem und nicht Text dar: tap, floppies, exhibits, artifacts, piracy, art, fidonet-on-the-internet
* Folgende Kategorien werden aus der Gesammtbetrachtung ausgeschlossen, da deren Charakter jener von Zeitschriften und nicht nutzergenerierter Inhalten entspricht: magazines, digest
* Dateien, deren Verhältnis von Text (inkl. Satz- und Leerzeichen) zu Dateilänge unter 0.95 sind, werden ausgeschlossen. Dies damit vorallem Fliesstext und Unterhaltungen bei der Analyse erfasst werden.
* Dateien, deren Länge zwischen 300 und 30'000 Zeichen liegen sollen in die Analyse eingeschlossen werden. Kürzere Texte erscheinen kaum aussagekräftig und längere Texte verzerren einerseits das Resultat enorm, andererseits erhöhen Sie die Dauer der Berechnungen in der Analyse enorm.

Mit der gewählten Filterung verbleiben 5'510 Textdateien die in den Textkorpus einfliessen.[^11] Der Gesammtumfang entspricht ca. 62 Millionen Zeichen. Diese Messwerte sind relevant, da damit die benötigte Rechnerleistung berechnet werden kann. (1GB Arbeitsspeicher pro 1 Million Zeichen) Zusätzlich zu den Textdateien wird die *Declaration* und einige charakteristische Textdateien als Datenset in den Textkorpus aufgenommen, damit mit diesen schliesslich verglichen werden kann.


## Analyse mittels NLP

Unter Natural Language Processing (NLP) werden Methoden zum maschinellen Analysieren, Modellieren und Verstehen von menschlicher Sprache verstanden [@vajjalaPracticalNaturalLanguage2020]. Allerdings ist dies nicht mit der maschinellen Erfassung von Sinn oder Bedeutung zu verwechseln, wie Emily M. Bender und Alexander Koller [-@benderClimbingNLUMeaning2020] argumentieren. Denn um ein solches Modell zu schaffen, wird eine Software mit einer grossen Menge an Daten trainiert und «if the training data is only form, there is not sufficient signal to learn the relation `M` between that form and the non-linguistic intent of human language users, nor `C` between form and the standing meaning the linguistic system assigns to each form.» [@benderClimbingNLUMeaning2020, 5187] Für gewisse Anwendungsbereiche von NLP hat dies ernstzunehmende Konsequenzen.[^9] Es ist aber gerade diese Eigenschaft, dass das Modell nur den Daten entspricht, mit welchen es gefüttert wird, die in der vorliegenden Arbeit zunutze gemacht wird. 

Aus dem bereinigten Datensatz soll ein Textkorpus generiert werden, welcher mithilfe der Python Bibliotheken «textacy», «scattertext» und «OCTIS» untersucht wird. Dabei werden folgende Analysen durchgeführt:

* Worthäufigkeiten absolut und Anzahl vorkommende Dateien: Gesammtdatensatz, Datensatz *Declaration*, pro Textfile-Kategorie
* Entitäten: Gesammtdatensatz, Datensatz *Declaration*
* Scattertext Analyse: Gesammtdatensatz verglichen mit *Declaration*
* Kategoriengrösse: Gesammtdatensatz
* Anzahl Dokumente pro Jahr: Gesammtdatensatz
* Topic Diversity mittels OCTIS (LDA und NeuralLDA): Gesammtdatensatz, Datensatz *Declaration*

Für die Auswertung mittels OCTIS [@terragniOCTISComparingOptimizing2021] wird auf Basis des Textkorpus, welcher für die anderen Analysen genutzt wurde, ein separater Datensatz erstellt. Die Tests werden mittels *OCTIS Dashboard* durchgeführt und die Analyse erfolgt mit den Modellen LDA [@srivastavaAutoencodingVariationalInference2017] und NeuralLDA [@bleiLatentDirichletAllocation2003].[^12]


[^7]: Dieser Schritt wurde im Mini-Projekt im Rahmen des Seminars \textquote{The ABC of Computational Text Analysis} durchgeführt.
[^8]: Die Protokollierung der manuellen Untersuchung des Datensatzes ist in *Anhang 1: Top100* und der Sourcecode in *Anhang 2: Sourcecode* zu finden. 
[^9]: Exemplarisch am \textquote{BERT neural network} wird das beschrieben im Artikel \textquote{Machines Beat Humans on a Reading Test. But Do They Understand?}, erschienen im Quanta Magazine [@pavlusMachinesBeatHumans2019].
[^10]: Dies ist in *Anhang 3: Textfiles.com Analysis* dokumentiert. 
[^11]: Die Bereinigung und Filterung des Datensatzes ist in *Anhang 4: Sourcecode Textfiles.com* dokumentert. Eine genauere Dokumentation inklusive des Textkorpuses ist online verfügbar: https://github.com/josiasbruderer/bbs-for-independence/ unter 03_workspace und 03_workspace/states
[^12]: Die Testresultate sind online verfügbar: https://github.com/josiasbruderer/bbs-for-independence/ unter 03_workspace/states